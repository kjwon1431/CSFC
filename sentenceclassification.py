# -*- coding: utf-8 -*-
"""SentenceClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PRFJBwTaRh4wvYKsQIcd3MyUmmzvIrz0
"""

from google.colab import drive 
drive.mount('/content/gdrive/')

import random

#트레인 데이터와 밸리드 데이터를 불러옴

import os
import numpy as np
import random

from torch.utils.data import Dataset
from torch.utils.data import DataLoader
data_review = os.path.join("./gdrive",'My Drive','dataset','train_data')
data_label = os.path.join("./gdrive",'My Drive','dataset','train_label')
fr=open(data_review, 'rt', encoding='utf-8')
fl=open(data_label,'rt',encoding = 'utf-8')
dr=fr.readlines()
dl=fl.readlines()

pdata=[]
s_u_m=0
for i in range(len(dr)):
  pdata.append((dr[i].split('\n')[0],dl[i].split('\n')[0]))
  s_u_m+=len(dr[i].split('\n')[0])
print(s_u_m//len(dr))
pdata

data_review = os.path.join("./gdrive",'My Drive','dataset','valid_data')
data_label = os.path.join("./gdrive",'My Drive','dataset','valid_label')
fr=open(data_review, 'rt', encoding='utf-8')
fl=open(data_label,'rt',encoding = 'utf-8')
dr=fr.readlines()
dl=fl.readlines()

vdata=[]
s_u_m=0
for i in range(len(dr)):
  vdata.append((dr[i].split('\n')[0],dl[i].split('\n')[0]))
  s_u_m+=len(dr[i].split('\n')[0])
vdata

num1=[]
num2=[]
num3=[]
num4=[]
num5=[]
num6=[]
num7=[]
num8=[]
num9=[]
num0=[]
for i in range(len(pdata)):
  if int(pdata[i][1])==1:
    num1.append(i)
  elif int(pdata[i][1])==2:
    num2.append(i)
  elif int(pdata[i][1])==3:
    num3.append(i)
  elif int(pdata[i][1])==4:
    num4.append(i)
  elif int(pdata[i][1])==5:
    num5.append(i)
  elif int(pdata[i][1])==6:
    num6.append(i)
  elif int(pdata[i][1])==7:
    num7.append(i)
  elif int(pdata[i][1])==8:
    num8.append(i)
  elif int(pdata[i][1])==9:
    num9.append(i)
  elif int(pdata[i][1])==10:
    num0.append(i)
print(num1[0],num2[0],num3[0],num4[0],num5[0],num6[0],num7[0],num8[0],num9[0],num0[0])

data_review = os.path.join("./gdrive",'My Drive','dataset','test_data')
data_label = os.path.join("./gdrive",'My Drive','dataset','valid_label')
fr=open(data_review, 'rt', encoding='utf-8')
fl=open(data_label,'rt',encoding = 'utf-8')
dr=fr.readlines()
dl=fl.readlines()
print(len(dr))
tdata=[]
s_u_m=0
for i in range(len(dr)):
  tdata.append((dr[i].split('\n')[0]))
  s_u_m+=len(dr[i].split('\n')[0])
tdata

#시험적인 CNN을 만들어 테스트를 진행함

import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 3x3 square convolution
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.conv2 = nn.Conv2d(6, 16, 3)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        print(x)
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        print(x)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))

        x = F.relu(self.fc2(x))
        x = self.fc3(x)

        return x
    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

#글자를 벡터화 시키는 char2vec함수, 그리고 그것을 기반으로 문장 전체를 벡터화시키는 함수

def char2vec(char):
  v=[]
  idx=ord(char)
  if ord('가')<= idx<=ord('힣'):
    f=idx - ord('가')
    s=f//28
    t=f%28
    f=s//21
    s=s%21
    t=t-1
    v.append(f)
    v.append(s)
    v.append(t)
  elif idx < 128:
    v.append(67+idx)
    v.append(-1)
    v.append(-1)
  elif ord('ㄱ')<idx<ord('ㅣ'):
    v.append(67+128+idx-12593)
    v.append(-1)
    v.append(-1)
  elif idx == ord('♡'):
    v.append(67 + 128 + 51)
    v.append(-1)
    v.append(-1)
  elif idx == ord('♥'):
    v.append(67 + 128 + 51)
    v.append(-1)
    v.append(-1)
  elif idx == ord('★'):
    v.append(67 + 128 + 51)
    v.append(-1)
    v.append(-1)
  elif idx == ord('☆'):
    v.append(67 + 128 + 51)
    v.append(-1)
    v.append(-1)
  else:
    v.append(-1)
    v.append(-1)
    v.append(-1)
  return v

print(char2vec("恨"))
print('a')


def str2vec(str,maxlen):
  v=[]
  if len(str)>=maxlen:
    l=maxlen
    
    for i in range(l):
      v.append(char2vec(str[i]))
  elif len(str)<maxlen:
    l=len(str)
    
    for i in range(l):
      v.append(char2vec(str[i]))
    for j in range(maxlen-l):
      v.append([-1,-1,-1])
  return torch.from_numpy(np.array(v)).float()

a=np.array(str2vec("가나다라",4),dtype=np.double)
print(a)
a=torch.from_numpy(a)
print(len(a.numpy()[0]))

#문장의 글자를 n글자씩 묶어서 문장벡터를 변형해주는 함수, 논문에 있는 내용을 참고함
def transformWithNgram(t,n):
  h,w=t.size()
  #a=t.numpy()
  res=[]
  
  for i in range(h+1-n):
    tmp=[]
    for j in range(n):
      for k in range(len(a[0])):
        tmp.append(int(t[i+j,k]))
    res.append(tmp)
  res=[[res]]
  return torch.from_numpy(np.array(res,dtype=np.double)).float()

test=transformWithNgram(a,3)
print(test.size())

#문장분류를 위한 CNN

class CFSC(nn.Module):
    def __init__(self):
        super(CFSC, self).__init__()
        self.conv3 = nn.Conv2d(1,10,(1,9))
        self.conv4 = nn.Conv2d(1,10,(1,12))
        self.conv5 = nn.Conv2d(1,10,(1,15))
        self.fc1=nn.Linear(30,64)
        
        self.Dropout=nn.Dropout(p=0.5)
        self.fc2=nn.Linear(64,32)
        self.fc3=nn.Linear(32,10)
    def forward(self,x):
        #x=x.cpu()
        a1,a2,a3,a4=x.size()
        x=x.view(a3,a4)
        
        input3=transformWithNgram(x,3)
        input4=transformWithNgram(x,4)
        input5=transformWithNgram(x,5)
        '''input3=input3.cuda()
        input4=input4.cuda()
        input5=input5.cuda()'''
        
        x=F.relu(self.conv3(input3))
        x1,x2,x3,x4=x.size()
        x=x.view(x1,x2,x4,x3)
        x=F.max_pool2d(x,(1,x3))
        x=x.view(-1,x2)

        y=F.relu(self.conv4(input4))
        y1,y2,y3,y4=y.size()
        y=y.view(y1,y2,y4,y3)
        y=F.max_pool2d(y,(1,y3))
        y=y.view(-1,y2)

        z=F.relu(self.conv5(input5))
        z1,z2,z3,z4=z.size()
        z=z.view(z1,z2,z4,z3)
        z=F.max_pool2d(z,(1,z3))
        z=z.view(-1,z2)

        out=torch.cat((x,y),0)
        out=torch.cat((out,z),0)
        out=out.view(-1,x2+y2+z2)
        out=self.Dropout(F.relu(self.fc1(out)))
        out=self.Dropout(F.relu(self.fc2(out)))
        out=self.fc3(out)
        
        return out

!nvidia-smi
!ls ./gdrive/'My Drive'

model=CFSC()
model.to(device)

path=os.path.join('./gdrive','My Drive','dataset','normal.pt')
model.load_state_dict(torch.load(path))

path=os.path.join('./gdrive','My Drive','dataset','Revise.pt')

torch.save(model.state_dict(),path)

path=os.path.join('./gdrive','My Drive','dataset','challenge.pt')

#model.load_state_dict(a)
for i in range(100):
  print('------------------------------------------------')
  print(i)
  print()
  criterion = nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.5)
  model = train_model(model,criterion,optimizer,scheduler,25)
  
  '''s_u_m=0
  model.eval()
  for i in range(10000):
    #print(i)
    with torch.no_grad():
      review,label=vdata[i]
      review=str2vec(review,33)
      h,w=review.size()
      review=review.view(1,1,h,w)
      review=review.cuda()
      #review.to(device)
      label_pred=model(review)
      #print(int(label))
      #print(int(label_pred.argmax(1))+1)
      if int(label_pred.argmax(1)+1)==int(label):
        #print(int(label))
        s_u_m+=1
    if i%10000==9999:
      print(i+1)
      #print(criterion(label_pred,torch.empty(1, dtype=torch.long).random_(5)))
  print(s_u_m/10000)
  print(s_u_m)'''
  

  torch.save(model.state_dict(),path)

def train_model(model,criterion,optimizer,scheduler,num_epochs):
  #since=time.time()

  for epoch in range(num_epochs):
    print('Epoch {}/{}'.format(epoch+1,num_epochs))
    
    model.train()
    print("trainning")
    for i in range(1000):
      if i%10==0:
        tmp=random.randrange(0,len(num1))
        idx=num1[tmp]
      elif i%10==1:
        tmp=random.randrange(0,len(num2))
        idx=num2[tmp]
      elif i%10==2:
        tmp=random.randrange(0,len(num3))
        idx=num3[tmp]
      elif i%10==3:
        tmp=random.randrange(0,len(num4))
        idx=num4[tmp]
      elif i%10==4:
        tmp=random.randrange(0,len(num5))
        idx=num5[tmp]
      elif i%10==5:
        tmp=random.randrange(0,len(num6))
        idx=num6[tmp]
      elif i%10==6:
        tmp=random.randrange(0,len(num7))
        idx=num7[tmp]
      elif i%10==7:
        tmp=random.randrange(0,len(num8))
        idx=num8[tmp]
      elif i%10==8:
        tmp=random.randrange(0,len(num9))
        idx=num9[tmp]
      elif i%10==9:
        tmp=random.randrange(0,len(num0))
        idx=num0[tmp]
      #idx=random.randrange(0,9000000)
      if i%1000==999:
        print(i+1)
      review = pdata[idx][0]
      review=str2vec(review,33)
      h,w=review.size()
      review=review.view(1,1,h,w)
      #review=review.cuda()
      #labels = torch.empty(1, dtype=torch.long).random_(5)
      labels=int(pdata[idx][1])-1
      tmp=torch.empty(1,dtype=torch.long)
      tmp[0]=labels
      labels=tmp
      #labels=labels.cuda()
      #print(type(labels))
      optimizer.zero_grad()
      with torch.set_grad_enabled(True):
        outputs=model(review)
      #print(outputs)
      
        loss=criterion(outputs,labels)
        loss.backward()
        optimizer.step()
    s_u_m=0
    model.eval()
    print("evaluating")
    for i in range(10000):
      #print(i)
      with torch.no_grad():
        review,label=vdata[i]
        review=str2vec(review,33)
        h,w=review.size()
        review=review.view(1,1,h,w)
        #review=review.cuda()
        #review.to(device)
        with torch.no_grad():
          label_pred=model(review)
        #print(int(label))
        #print(int(label_pred.argmax(1))+1)
        if int(label_pred.argmax(1)+1)==int(label):
          #print(int(label))
          s_u_m+=1
      if i%10000==9999:
        print(i+1)
        #print(criterion(label_pred,torch.empty(1, dtype=torch.long).random_(5)))
    print(s_u_m/10000)
    print(s_u_m)
    scheduler.step()
  return model

model = train_model(model,criterion,optimizer,scheduler,25)

import csv
a=os.path.join("./gdrive",'My Drive','dataset','outputFinal.csv')
f = open(a, 'w', encoding='utf-8')
wr = csv.writer(f)
#wr.writerow([1, "Alice", True])
#wr.writerow([2, "Bob", False])
#f.close()
s_u_m=0
#model.load_state_dict(torch.load(os.path.join("./gdrive",'My Drive','dataset','finalModel.pt')))
model.eval()
for i in range(400000):
  #print(i)
  with torch.set_grad_enabled(False):
    review=tdata[i]
    z=tdata[i]
    review=str2vec(review,33)
    h,w=review.size()
    review=review.view(1,1,h,w)
    label_pred=model(review)
    #print(type(int(label_pred.argmax(1)+1)))
    #break
    wr.writerow([int(label_pred.argmax(1)+1)])
    if int(label_pred.argmax(1)+1)==int(label):
      s_u_m+=1
  if i%1000==999:
    print(i+1)
    #print(criterion(label_pred,torch.empty(1, dtype=torch.long).random_(5)))
print(s_u_m/10000)
print(s_u_m)
f.close()

a=torch.load(os.path.join("./gdrive",'My Drive','dataset','normal.pt'))

s="가나다라"
type(list(s)[0])
ord(s[0])
cfsc=CFSC()
#print(cfsc(test.float()))

m=str2vec(pdata[0][0],33)
print(m)
q,w=m.size()
m=m.view(1,1,q,w)
m=cfsc(m)
print(m)

print(m.argmax(1))
#print(m.size())

#m=m.view(1,6,1,6)
#print(F.max_pool2d(m,(1,6)))

import csv
a=os.path.join("./gdrive",'My Drive','dataset','ot.csv')
f = open(a, 'w', encoding='utf-8')
wr = csv.writer(f)
wr.writerow([1, "Alice", True])
wr.writerow([2, "Bob", False])
f.close()